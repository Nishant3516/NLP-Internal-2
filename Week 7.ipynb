{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "Choose any corpus available on the internet freely. For the corpus, for each document, count how many times each stop word occurs and find out which are the most frequently occurring stop words. Further, calculate the term frequency and inverse document frequency as The motivation behind this is basically to find out how important a document is to a given query. For e.g.: If the query is say: “The brown crow”. “The” is less important. “Brown” and “crow” are relatively more important. Since “the” is a more common word, its tf will be high. Hence we multiply it by idf, by knowing how common it is to reduce its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\bansa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bansa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bansa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords, gutenberg\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 words: ['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "texts = gutenberg.sents('austen-emma.txt')\n",
    "\n",
    "flat_text = [word.lower() for sentence in texts for word in sentence]\n",
    "print(f\"First 50 words: {flat_text[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [\n",
    "    word for word in flat_text if word not in stop_words and word not in punctuation]\n",
    "word_frequency = Counter(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(corpus):\n",
    "    \"\"\"Compute Term Frequency for the corpus.\"\"\"\n",
    "    tf_dict = Counter(corpus)\n",
    "    total_terms = len(corpus)\n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] = tf_dict[word] / total_terms\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def compute_idf(corpus, documents):\n",
    "    \"\"\"Compute Inverse Document Frequency.\"\"\"\n",
    "    idf_dict = {}\n",
    "    total_docs = len(documents)\n",
    "    for word in set(corpus):\n",
    "        doc_count = sum(1 for doc in documents if word in doc)\n",
    "        idf_dict[word] = math.log(total_docs / (1 + doc_count))\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = compute_tf(filtered_text)\n",
    "documents = [filtered_text]\n",
    "\n",
    "idf = compute_idf(filtered_text, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop word counts in the original text (before filtering):\n",
      "a: 3129\n",
      "during: 17\n",
      "to: 5239\n",
      "he: 1806\n",
      "then: 169\n",
      "been: 759\n",
      "can: 284\n",
      "ours: 7\n",
      "no: 742\n",
      "such: 489\n",
      "itself: 15\n",
      "after: 161\n",
      "further: 1\n",
      "there: 549\n",
      "don: 16\n",
      "that: 1806\n",
      "i: 3178\n",
      "but: 1441\n",
      "whom: 73\n",
      "doing: 45\n",
      "what: 536\n",
      "re: 2\n",
      "yours: 5\n",
      "ourselves: 17\n",
      "himself: 146\n",
      "if: 485\n",
      "s: 935\n",
      "and: 4896\n",
      "at: 1031\n",
      "between: 73\n",
      "own: 301\n",
      "our: 97\n",
      "themselves: 40\n",
      "who: 294\n",
      "not: 2140\n",
      "should: 369\n",
      "have: 1320\n",
      "being: 358\n",
      "with: 1217\n",
      "very: 1202\n",
      "some: 262\n",
      "herself: 279\n",
      "has: 250\n",
      "before: 250\n",
      "were: 600\n",
      "was: 2398\n",
      "about: 249\n",
      "had: 1624\n",
      "the: 5201\n",
      "down: 70\n",
      "each: 46\n",
      "your: 364\n",
      "when: 363\n",
      "on: 692\n",
      "all: 845\n",
      "too: 254\n",
      "now: 309\n",
      "am: 425\n",
      "by: 571\n",
      "d: 12\n",
      "for: 1347\n",
      "against: 46\n",
      "it: 2528\n",
      "than: 415\n",
      "here: 154\n",
      "m: 2\n",
      "be: 1975\n",
      "ma: 12\n",
      "an: 464\n",
      "both: 85\n",
      "we: 349\n",
      "nor: 64\n",
      "does: 130\n",
      "off: 99\n",
      "where: 87\n",
      "only: 341\n",
      "is: 1240\n",
      "shan: 1\n",
      "under: 63\n",
      "him: 759\n",
      "out: 212\n",
      "o: 8\n",
      "those: 95\n",
      "just: 165\n",
      "them: 432\n",
      "her: 2469\n",
      "theirs: 1\n",
      "same: 102\n",
      "once: 83\n",
      "do: 640\n",
      "in: 2188\n",
      "more: 467\n",
      "will: 570\n",
      "my: 728\n",
      "myself: 103\n",
      "hers: 20\n",
      "which: 556\n",
      "from: 546\n",
      "few: 106\n",
      "t: 19\n",
      "its: 122\n",
      "into: 163\n",
      "again: 219\n",
      "so: 974\n",
      "having: 147\n",
      "she: 2340\n",
      "most: 248\n",
      "me: 573\n",
      "other: 221\n",
      "up: 190\n",
      "you: 1980\n",
      "his: 1145\n",
      "as: 1436\n",
      "or: 494\n",
      "over: 139\n",
      "because: 53\n",
      "they: 540\n",
      "yourself: 56\n",
      "of: 4291\n",
      "below: 4\n",
      "these: 66\n",
      "did: 352\n",
      "through: 61\n",
      "why: 55\n",
      "how: 371\n",
      "their: 306\n",
      "this: 526\n",
      "are: 455\n",
      "while: 133\n",
      "above: 12\n",
      "any: 654\n",
      "\n",
      "Most common stop words in the filtered text:\n",
      "to: 5239\n",
      "the: 5201\n",
      "and: 4896\n",
      "of: 4291\n",
      "i: 3178\n",
      "a: 3129\n",
      "it: 2528\n",
      "her: 2469\n",
      "was: 2398\n",
      "she: 2340\n",
      "\n",
      "First 10 words in the text with their raw frequency, TF, and IDF:\n",
      "\n",
      "Word: gentlemanlike\n",
      "Raw Frequency: 5\n",
      "Term Frequency (TF): 6.346949655995328e-05\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: orderly\n",
      "Raw Frequency: 1\n",
      "Term Frequency (TF): 1.2693899311990657e-05\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: alone\n",
      "Raw Frequency: 27\n",
      "Term Frequency (TF): 0.00034273528142374774\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: guidance\n",
      "Raw Frequency: 1\n",
      "Term Frequency (TF): 1.2693899311990657e-05\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: populous\n",
      "Raw Frequency: 2\n",
      "Term Frequency (TF): 2.5387798623981314e-05\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: inconsistency\n",
      "Raw Frequency: 2\n",
      "Term Frequency (TF): 2.5387798623981314e-05\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: distant\n",
      "Raw Frequency: 10\n",
      "Term Frequency (TF): 0.00012693899311990657\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: marrying\n",
      "Raw Frequency: 26\n",
      "Term Frequency (TF): 0.0003300413821117571\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: approve\n",
      "Raw Frequency: 7\n",
      "Term Frequency (TF): 8.885729518393461e-05\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n",
      "\n",
      "Word: guilty\n",
      "Raw Frequency: 1\n",
      "Term Frequency (TF): 1.2693899311990657e-05\n",
      "Inverse Document Frequency (IDF): -0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "stop_word_counts = Counter({word: flat_text.count(word)\n",
    "                           for word in stop_words if word in flat_text})\n",
    "\n",
    "print(\"\\nStop word counts in the original text (before filtering):\")\n",
    "for stop_word, count in stop_word_counts.items():\n",
    "    print(f\"{stop_word}: {count}\")\n",
    "\n",
    "most_common_stopwords = stop_word_counts.most_common(10)\n",
    "print(\"\\nMost common stop words in the filtered text:\")\n",
    "for stop_word, count in most_common_stopwords:\n",
    "    print(f\"{stop_word}: {count}\")\n",
    "\n",
    "print(\"\\nFirst 10 words in the text with their raw frequency, TF, and IDF:\")\n",
    "\n",
    "unique_words = list(set(filtered_text))[:10]\n",
    "\n",
    "for word in unique_words:\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"Raw Frequency: {word_frequency[word]}\")\n",
    "    print(f\"Term Frequency (TF): {tf.get(word, 0)}\")\n",
    "    print(f\"Inverse Document Frequency (IDF): {idf.get(word, 0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
